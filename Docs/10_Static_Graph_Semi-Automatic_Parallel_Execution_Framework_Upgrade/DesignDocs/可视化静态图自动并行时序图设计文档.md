# 可视化静态图自动并行时序图

由于当下大模型的训练时间较长，分布式训练时序图的可视化对于调试和分析模型的训练过程非常重要。当下没有工具能够直接给出各个GPU设备上不同Job的运行区间，因此我们需要设计一个可视化工具来实现这个功能。

静态图模式下自动并行的运行将调用C++端 `StandaloneExecutor::Run` ，该方法中将顺序执行提前拆分好的Job。可视化静态图自动并行时序图的主要目的是将不同设备上Job的运行时序图可视化出来并使用 `Chrome::tracing` 查看。

## 方案设计

###  Step1: FLAGS控制可视化开关

首先添加一个FLAGS `FLAGS_auto_parallel_profiler` 用于控制可视化时序图的开关。

后期，改成解析命令行参数，如 `python -m paddle.distributed.launch --visualize_pipline=True train.py`。

### Step2: C++端添加相关VLOG信息

静态图模式下自动并行的运行将调用C++端 `StandaloneExecutor::Run` ，该方法中将顺序执行提前拆分好的Job，相关代码如下：

```c++
paddle::framework::FetchList StandaloneExecutor::Run(
    const std::vector<std::string>& feed_names) {
  platform::RecordEvent record_event(
      "StandaloneExecutor::run", platform::TracerEventType::UserDefined, 1);

  const auto& jobs = plan_.JobList();
  ...
  for (size_t job_idx = 0; job_idx < jobs.size(); ++job_idx) {
    const auto& job = jobs[job_idx];
    const std::string& job_type = job->Type();
    ...
    if (jobs.size() 1 && job_type != "forward") {
      const std::vector<std::stringtmp_feed_names = {};
      interpretercores_[job_idx]->Run(tmp_feed_names, /*need_fetch = */ false);
    } else {
      interpretercores_[job_idx]->Run(feed_names, /*need_fetch = */ false);
    }
  }
  ...
}
```
其中 `job_type` 包含 `forward`、`backward` 、`lr`、`optimizer`、`default`。
由于Run里面的Op是多线程执行的，输出 job 结束时，在 Run 里启动的 op 可能还没有执行完毕。因此我们在每个 Job 开始和结束的时候插入多个 `cudaEventRecord` （每个流插入一个），这样就能保证每个 Job 的开始和结束时间都是准确的。并通过 `cudaEventSynchronize` 等待所有的 Job 结束后，再通过 `cudaEventElapsedTime` 计算出每个 Job 的运行时间。
 `FLAGS_auto_parallel_profiler` 为 True 时，在所有Job运行结束后遍历所有的Job，输出每个Job的开始和结束时间，以及Job的类型。


```c++
// record each job's run time
  if (FLAGS_auto_parallel_profiler) {
    for (size_t job_idx = 0; job_idx < jobs.size(); ++job_idx) {
      const auto& job = jobs[job_idx];
      const std::string& job_type = job->Type();
      double start_time, end_time;
      std::tie(start_time, end_time) =
          interpretercores_[job_idx]->InterpreterRunTime();
      VLOG(0) << "Profiler Info: Job (" << job_idx << "), type = " << job_type
              << ", micro_batch_id = " << job->MicroBatchId()
              << ", job_start_time = " << std::to_string(start_time)
              << ", job_end_time = " << std::to_string(end_time);
    }
```

### Step3: Python 端解析输出的log日志

在自动并行模式下，log目录下会输出n个日志文件（n=设备数）。在训练结束后通过独立脚本解析这些日志文件，得到每个Job的开始和结束时间，以及Job的类型。

####  正则匹配日志

在 C++ 端输出的日志格式如下：

```
I1020 09:15:07.265326 22317 standalone_executor.cc:217] Profiler Info: Job (3), type = forward, micro_batch_id = 2, job_start_time = 1697793307213.760986, job_end_time = 1697793307254.006104
I1020 09:15:07.265338 22317 standalone_executor.cc:217] Profiler Info: Job (4), type = forward, micro_batch_id = 3, job_start_time = 1697793307214.742920, job_end_time = 1697793307219.168945
```

我们可以通过如下正则表达式解析出所有类似的日志，并获取到时间、job_id、job_type、起始信息等：

```python
log_pattern = r'.*?Profiler Info: Job \((\d+)\), type = (\w+), micro_batch_id = (\d+), job_start_time = (\d+.\d+), job_end_time = (\d+.\d+)'
    matches = re.findall(log_pattern, log_data)
```

##### 整合日志格式为json

在解析出所有的日志信息后，我们需要将其整合为json格式，方便后续使用Chrome tracing进行可视化。

Chrome Tracing的数据格式如下：

```json
{
  "traceEvents": [
    {
      "name": "A",
      "cat": "F_1",
      "ph": "B",
      "ts": 10,
      "pid": 1,
      "tid": 1
    },
    {
      "name": "B",
      "cat": "F_1",
      "ph": "E",
      "ts": 20,
      "pid": 1,
      "tid": 1
    }
  ]
}
```

其中 `name` 为事件名称，`cat` 为事件类别，`ph` 为事件类型，`ts` 为事件发生的时间戳，`pid` 为进程ID，`tid` 为线程ID。在可视化的时候可以固定 `pid` 用 `tid` 区分不同的设备。

可以通过如下代码将日志信息整合为json格式：

```python
def process_log_data(log_data, device_id):
    log_pattern = r'.*?Profiler Info: Job \((\d+)\), type = (\w+), micro_batch_id = (\d+), job_start_time = (\d+.\d+), job_end_time = (\d+.\d+)'
    matches = re.findall(log_pattern, log_data)
    events = []
    color_map = {
        "forward": "thread_state_running",  # RGB: 126, 200, 148
        "backward": "rail_idle",  # RGB: 238, 142, 0
        "optimizer": "rail_response",  # RGB: 238, 142, 0
        "default": "thread_state_unknown",  # RGB: 199, 155, 125
    }
    for match in matches:
        job_id, job_type, micro_batch_id, job_start_time, job_end_time = match
        # skip lr job
        if job_type in ["lr"]:
            continue

        event_start = {
            "name": job_type[0].upper() + "_" + str(job_id),
            "cat": job_type,
            "ph": "B",
            "ts": float(job_start_time.strip()) * 1000,
            "pid": "Main",
            "tid": "GPU" + str(device_id),
            "cname": color_map[job_type],
        }
        event_end = {
            "name": job_type[0].upper() + "_" + str(job_id),
            "cat": job_type,
            "ph": "E",
            "pid": "Main",
            "ts": float(job_end_time.strip()) * 1000,
            "tid": "GPU" + str(device_id),
            "cname": color_map[job_type],
        }
        events.append(event_start)
        events.append(event_end)

    return events
```

上面的代码一次可以解析一个log_file的日志信息，通过将多个设备的日志信息整合为一个json文件，即可以使用Chrome Tracing进行可视化。

##### 支持Perfetto格式

Perfetto 是 chrome::tracing 的升级版，具有更好看的界面和更多的功能。因此我们也支持将日志信息整合为Perfetto格式。

Perfetto 基本兼容 chrome::tracing 格式的 json 文件，但是 Perfetto 格式需要指定线程的名称，且tid不能为字符串，因此我们需要对获取到的json格式数据进行一些处理。

```python
# support Perfetto format
save_path = os.path.join(args.log_dir, "pipeline_profile_perfetto.json")
# add thread name
for i in range(len(args.devices.split(","))):
    all_events.extend(
        [
            {
                "args": {"name": "GPU"},
                "cat": "__metadata",
                "name": "thread_name",
                "ph": "M",
                "pid": "Main",
                "tid": i,
                "ts": 0,
            }
        ]
    )
json_str = json.dumps({"traceEvents": all_events})
# replace tid to int
for i in range(len(args.devices.split(","))):
    json_str = json_str.replace(f'"GPU{i}"', f'{i}')
with open(save_path, "w") as f:
    f.write(json_str)
```


## 如何使用？

以下以使用 test_pipeline_scheduler 单侧生成日志文件并生成可视化时序图为例进行说明：

由于单侧默认会清空掉生成的日志文件，我们需要先将清空日志的逻辑删除并指定log文件夹：

```python
class TestFThenBPass(unittest.TestCase):
    def test_pp2(self):
        file_dir = os.path.dirname(os.path.abspath(__file__))
        launch_model_path = os.path.join(
            file_dir, "pipeline_scheduler_unittest.py"
        )

        if os.environ.get("WITH_COVERAGE", "OFF") == "ON":
            coverage_args = ["-m", "coverage", "run", "--branch", "-p"]
        else:
            coverage_args = []

        # tmp_dir = tempfile.TemporaryDirectory()
        cmd = (
            [sys.executable, "-u"]
            + coverage_args
            + [
                "-m",
                "paddle.distributed.launch",
                "--devices",
                "0,1",
                "--log_dir",
                "/home/root/Paddle/build/Testing/Temporary",
                launch_model_path,
            ]
        )

        process = subprocess.Popen(cmd)
        process.wait()
        self.assertEqual(process.returncode, 0)

        # tmp_dir.cleanup()
```

1、在开启FLAG的前提下，运行训练过程并生成log

```bash
FLAGS_auto_parallel_profiler=1 GLOG_v=0 ctest -R test_pipeline_scheduler $VV
```

GLOG_v=0 的目的是产生尽可能少的日志，降低正则匹配的时间。

2、解析日志并生成可视化时序图

```bash
python python/paddle/distributed/fleet/meta_parallel/pp_utils/profiler_helper_static.py --devices 0,1 --log_dir build/Testing/Temporary
```

3、使用Chrome Tracing进行可视化

![picture 0](images/a82238a7baf521dd7b2e2e32564d6d0fd846c706d4233c637670986f84244e04.png)  

也可以使用 perfetto 打开 pipeline_profile_perfetto.json

![picture 1](images/8f1babb4800bbfa02324e3dabbba640766e6db317e66c18edafae0999dfdb9fd.png)  
